% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/RcppExports.R
\name{solve_mdp_rand}
\alias{solve_mdp_rand}
\title{Solves a plain Markov decision process with randomized policies.}
\usage{
solve_mdp_rand(
  mdp,
  discount,
  algorithm = "mpi",
  policy_fixed = NULL,
  maxresidual = 0.001,
  iterations = 10000L,
  timeout = 300,
  value_init = NULL,
  show_progress = 1L
)
}
\arguments{
\item{mdp}{A dataframe representation of the MDP. Each row
represents a single transition from one state to another
after taking an action a. The columns are:
idstatefrom, idaction, idstateto, probability, reward}

\item{discount}{Discount factor in [0,1]}

\item{algorithm}{One of "mpi", "vi", "vi_j", "vi_g", "pi"}

\item{policy_fixed}{States for which the  policy should be fixed. This
should be a dataframe with columns idstate, idaction, probability.
 The policy is optimized only for states that are missing, and the
 fixed policy is used otherwise}

\item{maxresidual}{Residual at which to terminate}

\item{iterations}{Maximum number of iterations}

\item{timeout}{Maximum number of secods for which to run the computation}

\item{value_init}{A  dataframe that contains the initial value function used
to initialize the method. The columns should be idstate and value.
Any states that are not provided are initialized to 0.}

\item{show_progress}{Whether to show a progress bar during the computation
0 means no progress, 1 is progress bar, and 2 is a detailed report}
}
\value{
A list with value function policy and other values
}
\description{
The method can be provided with a randomized policy for some states
and the output policy is randomized.
}

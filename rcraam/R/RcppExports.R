# Generated by using Rcpp::compileAttributes() -> do not edit by hand
# Generator token: 10BE3573-1514-4C36-9D1C-5A225CD40393

#' Computes the maximum distribution subject to L1 constraints
#'
#' @param value Random variable (objective)
#' @param reference_dst Reference distribution of the same size as value
#' @param budget Maximum L1 distance from the reference dst
#'
#' @returns A list with dst as the worstcase distribution,
#'         and value as the objective
worstcase_l1 <- function(value, reference_dst, budget) {
    .Call(`_rcraam_worstcase_l1`, value, reference_dst, budget)
}

#' Computes the maximum distribution subject to weighted L1 constraints
#'
#' @param value Random variable (objective)
#' @param reference_dst Reference distribution of the same size as value
#' @param budget Maximum L1 distance from the reference dst
#' @param w set of weights for ambiguity set
#'
#' @returns A list with dst as the worstcase distribution,
#'         and value as the objective
worstcase_l1_w <- function(value, reference_dst, w, budget) {
    .Call(`_rcraam_worstcase_l1_w`, value, reference_dst, w, budget)
}

#' Computes the maximum distribution subject to weighted L1 constraints using Gurobi
#'
#' The function is only supported when the package is installed with Gurobi support 
#'
#' @param value Random variable (objective)
#' @param reference_dst Reference distribution of the same size as value
#' @param budget Maximum L1 distance from the reference dst
#' @param w set of weights for ambiguity set
#'
#' @returns A list with dst as the worstcase distribution,
#'         and value as the objective
worstcase_l1_w_gurobi <- function(value, reference_dst, w, budget) {
    .Call(`_rcraam_worstcase_l1_w_gurobi`, value, reference_dst, w, budget)
}

#' Computes the maximum distribution subject to weighted Linf constraints using Gurobi
#'
#' The function is only supported when the package is installed with Gurobi support 
#'
#' @param value Random variable (objective)
#' @param reference_dst Reference distribution of the same size as value
#' @param budget Maximum Linf distance from the reference dst
#' @param w set of weights for ambiguity set
#'
#' @returns A list with dst as the worstcase distribution,
#'         and value as the objective
worstcase_linf_w_gurobi <- function(value, reference_dst, w, budget) {
    .Call(`_rcraam_worstcase_linf_w_gurobi`, value, reference_dst, w, budget)
}

#' Computes average value at risk
#'
#' @param value Random variable (as a vector over realizations)
#' @param reference_dst Reference distribution of the same size as value
#' @param alpha Confidence value. 0 is worst case, 1 is average
#'
#' @returns A list with dst as the distorted distribution,
#'          and value as the avar value
avar <- function(value, reference_dst, alpha) {
    .Call(`_rcraam_avar`, value, reference_dst, alpha)
}

pack_actions <- function(mdp) {
    .Call(`_rcraam_pack_actions`, mdp)
}

mdp_clean <- function(mdp) {
    .Call(`_rcraam_mdp_clean`, mdp)
}

#' Solves a plain Markov decision process.
#'
#' This method supports only deterministic policies. See solve_mdp_rand for a
#' method that supports randomized policies.
#'
#' @param mdp A dataframe representation of the MDP. Each row
#'            represents a single transition from one state to another
#'            after taking an action a. The columns are:
#'            idstatefrom, idaction, idstateto, probability, reward
#' @param discount Discount factor in [0,1]
#' @param algorithm One of "mpi", "vi", "vi_j", "vi_g", "pi". Also supports "lp"
#'           when Gurobi is properly installed
#' @param policy_fixed States for which the  policy should be fixed. This
#'          should be a dataframe with columns idstate and idaction. The policy
#'          is optimized only for states that are missing, and the fixed policy
#'          is used otherwise. Both indices are 0-based.
#' @param maxresidual Residual at which to terminate
#' @param iterations Maximum number of iterations
#' @param timeout Maximum number of secods for which to run the computation
#' @param value_init A  dataframe that contains the initial value function used
#'          to initialize the method. The columns should be idstate and value.
#'          Any states that are not provided are initialized to 0.
#' @param pack_actions Whether to remove actions with no transition probabilities,
#'          and rename others for the same state to prevent gaps. The policy
#'          for the original actions can be recovered using ``action_map'' frame
#'          in the result
#' @param show_progress Whether to show a progress bar during the computation.
#'         0 means no progress, 1 is progress bar, and 2 is a detailed report
#' @return A list with value function policy and other values
solve_mdp <- function(mdp, discount, algorithm = "mpi", policy_fixed = NULL, maxresidual = 10e-4, iterations = 10000L, timeout = 300, value_init = NULL, pack_actions = FALSE, show_progress = 1L) {
    .Call(`_rcraam_solve_mdp`, mdp, discount, algorithm, policy_fixed, maxresidual, iterations, timeout, value_init, pack_actions, show_progress)
}

#' Solves a plain Markov decision process with randomized policies.
#'
#' The method can be provided with a randomized policy for some states
#' and the output policy is randomized.
#'
#' @param mdp A dataframe representation of the MDP. Each row
#'            represents a single transition from one state to another
#'            after taking an action a. The columns are:
#'            idstatefrom, idaction, idstateto, probability, reward
#' @param discount Discount factor in [0,1]
#' @param algorithm One of "mpi", "vi", "vi_j", "vi_g", "pi"
#' @param policy_fixed States for which the  policy should be fixed. This
#'         should be a dataframe with columns idstate, idaction, probability.
#'          The policy is optimized only for states that are missing, and the
#'          fixed policy is used otherwise
#' @param maxresidual Residual at which to terminate
#' @param iterations Maximum number of iterations
#' @param timeout Maximum number of secods for which to run the computation
#' @param value_init A  dataframe that contains the initial value function used
#'          to initialize the method. The columns should be idstate and value.
#'          Any states that are not provided are initialized to 0.
#' @param show_progress Whether to show a progress bar during the computation
#'         0 means no progress, 1 is progress bar, and 2 is a detailed report
#'
#' @return A list with value function policy and other values
solve_mdp_rand <- function(mdp, discount, algorithm = "mpi", policy_fixed = NULL, maxresidual = 10e-4, iterations = 10000L, timeout = 300, value_init = NULL, show_progress = 1L) {
    .Call(`_rcraam_solve_mdp_rand`, mdp, discount, algorithm, policy_fixed, maxresidual, iterations, timeout, value_init, show_progress)
}

#' Computes the function for the MDP for the given value function and discount factor
#'
#' @param mdp A dataframe representation of the MDP. Each row
#'            represents a single transition from one state to another
#'            after taking an action a. The columns are:
#'            idstatefrom, idaction, idstateto, probability, reward
#' @param discount Discount factor in [0,1]
#' @param valuefunction A dataframe representation of the value function. Each row
#'             represents a state. The columns must be idstate, value
#'
#' @return Dataframe with idstate, idaction, qvalue columns
compute_qvalues <- function(mdp, discount, valuefunction) {
    .Call(`_rcraam_compute_qvalues`, mdp, discount, valuefunction)
}

#' Solves a robust Markov decision process with state-action rectangular
#' ambiguity sets. The worst-case is computed with the MDP transition
#' probabilities treated as nominal values.
#'
#' NOTE: The algorithms: pi, mpi may cycle infinitely without converging to a solution,
#' when solving a robust MDP.
#' The algorithms ppi and mppi are guaranteed to converge to an optimal solution.
#'
#' Important: Worst-case transitions are allowed only to idstateto states that
#' are provided in the mdp dataframe, even when the transition
#' probability to those states is 0.
#'
#' @param mdp A dataframe representation of the MDP. Each row
#'            represents a single transition from one state to another
#'            after taking an action a. The columns are:
#'            idstatefrom, idaction, idstateto, probability, reward
#' @param discount Discount factor in [0,1]
#' @param nature Algorithm used to select the robust outcome. See details for options.
#' @param nature_par Parameters for the nature. Varies depending on the nature.
#'                   See details for options.
#' @param algorithm One of "ppi", "mppi", "vppi", "mpi", "vi", "vi_j", "vi_g", "pi". MPI and PI may
#'           may not converge
#' @param policy_fixed States for which the  policy should be fixed. This
#'          should be a dataframe with columns idstate and idaction. The policy
#'          is optimized only for states that are missing, and the fixed policy
#'          is used otherwise
#' @param maxresidual Residual at which to terminate
#' @param iterations Maximum number of iterations
#' @param timeout Maximum number of secods for which to run the computation
#' @param value_init A  dataframe that contains the initial value function used
#'          to initialize the method. The columns should be idstate and value.
#'          Any states that are not provided are initialized to 0.
#' @param pack_actions Whether to remove actions with no transition probabilities,
#'          and rename others for the same state to prevent gaps. The policy
#'          for the original actions can be recovered using ``action_map'' frame
#'          in the result
#' @param output_tran Whether to construct and return a matrix of transition
#'          probabilites and a vector of rewards
#' @param show_progress Whether to show a progress bar during the computation.
#'         0 means no progress, 1 is progress bar, and 2 is a detailed report
#'
#' @return A list with value function policy and other values
#'
#' @details
#'
#' The options for nature and the corresponding nature_par are:
#'    \itemize{
#'         \item "l1u" an l1 ambiguity set with the same budget for all s,a.
#'                nature_par is a float number representing the budget
#'         \item "l1" an ambiguity set with different budgets for each s,a.
#'                nature_par is dataframe with idstate, idaction, budget
#'         \item "l1w" an l1-weighted ambiguity set with different weights
#'                      and budgets for each state and action
#'                 nature_par is a list with two elements: budgets, weights.
#'                 budgets must be a dataframe with columns idstate, idaction, budget
#'                 and weights must be a dataframe with columns:
#'                 idstatefrom, idaction, idstateto, weight (for the l1 weighted norms)
#'         \item "evaru" a convex combination of expectation and V@R over
#'                 transition probabilites. Uniform over all states and actions
#'                 nature_par is a list with parameters (alpha, beta). The worst-case
#'                 response is computed as:
#'                 beta * var [z] + (1-beta) * E[z], where
#'                 var is inf{x in R : P[X <= x] >= alpha}, with alpha = 0 being the
#'                 worst-case.
#'         \item "evaru" a convex combination of expectation and AV@R over
#'                 transition probabilites. Uniform over states
#'                 nature_par is a list with parameters (alpha, beta). The worst-case
#'                 response is computed as:
#'                 beta * var [z] + (1-beta) * E[z], where
#'                 var is AVaR(z,alpha) = 1/alpha * ( E[X I{X <= x_a} ] + x_a (alpha - P[X <= x_a])
#'                 where I is the indicator function and
#'                 x_a = inf{x in R : P[X <= x] >= alpha} being the
#'                 worst-case.
#'    }
rsolve_mdp_sa <- function(mdp, discount, nature, nature_par, algorithm = "mppi", policy_fixed = NULL, maxresidual = 10e-4, iterations = 10000L, timeout = 300, value_init = NULL, pack_actions = FALSE, output_tran = FALSE, show_progress = 1L) {
    .Call(`_rcraam_rsolve_mdp_sa`, mdp, discount, nature, nature_par, algorithm, policy_fixed, maxresidual, iterations, timeout, value_init, pack_actions, output_tran, show_progress)
}

#' Solves a robust Markov decision process with state-action rectangular
#' ambiguity sets.
#'
#' The worst-case is computed across the outcomes and not
#' the actual transition probabilities.
#'
#' NOTE: The algorithms  mpi and pi may cycle infinitely without converging to a solution,
#' when solving a robust MDP.
#' The algorithms ppi and mppi are guaranteed to converge to an optimal solution.
#'
#'
#' @param algorithm One of "ppi", "mppi", "mpi", "vi", "vi_j", "vi_g", "pi". MPI may
#'           may not converge
#' @param policy_fixed States for which the  policy should be fixed. This
#'          should be a dataframe with columns idstate and idaction. The policy
#'          is optimized only for states that are missing, and the fixed policy
#'          is used otherwise
#' @param maxresidual Residual at which to terminate
#' @param iterations Maximum number of iterations
#' @param timeout Maximum number of secods for which to run the computation
#' @param value_init A  dataframe that contains the initial value function used
#'          to initialize the method. The columns should be idstate and value.
#'          Any states that are not provided are initialized to 0.
#' @param pack_actions Whether to remove actions with no transition probabilities,
#'          and rename others for the same state to prevent gaps. The policy
#'          for the original actions can be recovered using ``action_map'' frame
#'          in the result
#' @param output_tran Whether to construct and return a matrix of transition
#'          probabilites and a vector of rewards
#' @param show_progress Whether to show a progress bar during the computation.
#'         0 means no progress, 1 is progress bar, and 2 is a detailed report
#'
#' @return A list with value function policy and other values
#'
#' @details
#'
#' The options for nature and the corresponding nature_par are:
#'    \itemize{
#'         \item "exp" plain expectation over the outcomes
#'         \item "evaru" a convex combination of expectation and V@R over
#'                 transition probabilites. Uniform over all states and actions
#'                 nature_par is a list with parameters (alpha, beta). The worst-case
#'                 response is computed as:
#'                 beta * var [z] + (1-beta) * E[z], where
#'                 var is \eqn{VaR(z,\alpha) = \inf{x \in R : P[X <= x] >= \alpha}}, with \eqn{\alpha = 0} being the
#'                 worst-case.
#'         \item "eavaru" a convex combination of expectation and AV@R over
#'                 transition probabilites. Uniform over states
#'                 nature_par is a list with parameters (alpha, beta). The worst-case
#'                 response is computed as:
#'                 beta * var [z] + (1-beta) * E[z], where
#'                 var is \eqn{AVaR(z,alpha) =  1/alpha * ( E[X I{X <= x_a} ] + x_a (alpha - P[X <= x_a] )}
#'                 where I is the indicator function and
#'                 \eqn{x_a = \inf{x \in R : P[X <= x] >= \alpha}} being the
#'                 worst-case.
#'    }
rsolve_mdpo_sa <- function(mdpo, discount, nature, nature_par, algorithm = "mppi", policy_fixed = NULL, maxresidual = 10e-4, iterations = 10000L, timeout = 300, value_init = NULL, pack_actions = FALSE, output_tran = FALSE, show_progress = 1L) {
    .Call(`_rcraam_rsolve_mdpo_sa`, mdpo, discount, nature, nature_par, algorithm, policy_fixed, maxresidual, iterations, timeout, value_init, pack_actions, output_tran, show_progress)
}

#' Solves an MDPO with static uncertainty using a non-convex global optimization method.
#'
#' The objective is:
#'  \deqn{\max_{\pi} \beta * CVaR_{P \sim f}^\alpha [return(\pi,P)] + (1-\beta) * E_{P \sim f}^\alpha [return(\pi,P)]}
#'
#' @param mdpo Uncertain MDP. The outcomes are assumed to represent the uncertainty over MDPs.
#'              The number of outcomes must be uniform for all states and actions
#'              (except for terminal states which have no actions).
#' @param alpha Risk level of avar (0 = worst-case). The minimum value is 1e-5, the maximum
#'              value is 1.
#' @param beta Weight on AVaR and the complement (1-beta) is the weight
#'              on the expectation term. The value must be between 0 and 1.
#' @param discount Discount factor. Clamped to be in [0,1]
#' @param init_distribution Initial distribution over states. The columns should be
#'                             are idstate, and probability.
#' @param model_distribution Distribution over the models. The default is empty, which translates
#'                   to a uniform distribution. The columns should be idstate, and probablity.
#' @param output_filename Name of the file to save the model output. Valid suffixes are
#'                          .mps, .rew, .lp, or .rlp for writing the model itself.
#'                        If it is an empty string, then it does not write the file.
srsolve_mdpo <- function(mdpo, init_distribution, discount, alpha, beta, algorithm = "milp", model_distribution = NULL, output_filename = "") {
    .Call(`_rcraam_srsolve_mdpo`, mdpo, init_distribution, discount, alpha, beta, algorithm, model_distribution, output_filename)
}

#' Solves a robust Markov decision process with state rectangular
#' ambiguity sets. The worst-case is computed with the MDP transition
#' probabilities treated as nominal values.
#'
#' NOTE: The algorithms: pi, mpi may cycle infinitely without converging to a solution,
#' when solving a robust MDP.
#' The algorithms ppi and mppi are guaranteed to converge to an optimal solution.
#'
#' Important: Worst-case transitions are allowed only to idstateto states that
#' are provided in the mdp dataframe, even when the transition
#' probability to those states is 0.
#'
#' @param algorithm One of "ppi", "mppi", "mpi", "vi", "vi_j", "v_g", "pi". MPI may
#'           may not converge
#' @param policy_fixed States for which the  policy should be fixed. This
#'          should be a dataframe with columns idstate and idaction. The policy
#'          is optimized only for states that are missing, and the fixed policy
#'          is used otherwise
#' @param maxresidual Residual at which to terminate
#' @param iterations Maximum number of iterations
#' @param timeout Maximum number of secods for which to run the computation
#' @param value_init A  dataframe that contains the initial value function used
#'          to initialize the method. The columns should be idstate and value.
#'          Any states that are not provided are initialized to 0.
#' @param pack_actions Whether to remove actions with no transition probabilities,
#'          and rename others for the same state to prevent gaps. The policy
#'          for the original actions can be recovered using ``action_map'' frame
#'          in the result
#' @param output_tran Whether to construct and return a matrix of transition
#'          probabilites and a vector of rewards
#' @param show_progress Whether to show a progress bar during the computation.
#'         0 means no progress, 1 is progress bar, and 2 is a detailed report
#'
#' @return A list with value function policy and other values
#' @details
#'
#' The options for nature and the corresponding nature_par are:
#'    \itemize{
#'         \item "l1u" an l1 ambiguity set with the same budget for all s.
#'                nature_par is a float number representing the budget
#'         \item "l1" an ambiguity set with different budgets for each s.
#'                nature_par is dataframe with idstate, budget
#'         \item "l1w" an l1-weighted ambiguity set with different weights
#'                      and budgets for each state and action
#'                 nature_par is a list with two elements: budgets, weights.
#'                 budgets must be a dataframe with columns idstate, budget
#'                 and weights must be a dataframe with columns:
#'                 idstatefrom, idaction, idstateto, weight (for the l1 weighted norms)
#'    }
rsolve_mdp_s <- function(mdp, discount, nature, nature_par, algorithm = "mppi", policy_fixed = NULL, maxresidual = 10e-4, iterations = 10000L, timeout = 300, value_init = NULL, pack_actions = FALSE, output_tran = FALSE, show_progress = 1L) {
    .Call(`_rcraam_rsolve_mdp_s`, mdp, discount, nature, nature_par, algorithm, policy_fixed, maxresidual, iterations, timeout, value_init, pack_actions, output_tran, show_progress)
}

#' Solves a robust Markov decision process with state-action rectangular
#' ambiguity sets.
#'
#' The worst-case is computed across the outcomes and not
#' the actual transition probabilities.
#'
#' NOTE: The algorithms  mpi and pi may cycle infinitely without converging to a solution,
#' when solving a robust MDP.
#' The algorithms ppi and mppi are guaranteed to converge to an optimal solution.
#'
#'
#' @param algorithm One of "ppi", "mppi", "mpi", "vi", "vi_j", "vi_g", "pi". MPI may
#'           may not converge
#' @param policy_fixed States for which the  policy should be fixed. This
#'          should be a dataframe with columns idstate and idaction. The policy
#'          is optimized only for states that are missing, and the fixed policy
#'          is used otherwise
#' @param maxresidual Residual at which to terminate
#' @param iterations Maximum number of iterations
#' @param timeout Maximum number of secods for which to run the computation
#' @param value_init A  dataframe that contains the initial value function used
#'          to initialize the method. The columns should be idstate and value.
#'          Any states that are not provided are initialized to 0.
#' @param pack_actions Whether to remove actions with no transition probabilities,
#'          and rename others for the same state to prevent gaps. The policy
#'          for the original actions can be recovered using ``action_map'' frame
#'          in the result
#' @param output_tran Whether to construct and return a matrix of transition
#'          probabilites and a vector of rewards
#' @param show_progress Whether to show a progress bar during the computation.
#'         0 means no progress, 1 is progress bar, and 2 is a detailed report
#'
#' @return A list with value function policy and other values
#'
#' @details
#'
#' The options for nature and the corresponding nature_par are:
#'    \itemize{
#'         \item "exp" plain expectation over the outcomes
#'         \item "eavaru" a convex combination of expectation and AV@R over
#'                 transition probabilites. Uniform over states
#'                 nature_par is a list with parameters (alpha, beta). The worst-case
#'                 response is computed as:
#'                 beta * var [z] + (1-beta) * E[z], where
#'                 var is \eqn{AVaR(z,alpha) =  1/alpha * ( E[X I{X <= x_a} ] + x_a (alpha - P[X <= x_a] )}
#'                 where I is the indicator function and
#'                 \eqn{x_a = \inf{x \in R : P[X <= x] >= \alpha}} being the
#'                 worst-case.
#'    }
rsolve_mdpo_s <- function(mdpo, discount, nature, nature_par, algorithm = "mppi", policy_fixed = NULL, maxresidual = 10e-4, iterations = 10000L, timeout = 300, value_init = NULL, pack_actions = FALSE, output_tran = FALSE, show_progress = 1L) {
    .Call(`_rcraam_rsolve_mdpo_s`, mdpo, discount, nature, nature_par, algorithm, policy_fixed, maxresidual, iterations, timeout, value_init, pack_actions, output_tran, show_progress)
}

set_rcraam_threads <- function(n) {
    invisible(.Call(`_rcraam_set_rcraam_threads`, n))
}

#' Sets a gurobi parameter. Even numeric values may be provided as strings
#'
#' See https://www.gurobi.com/wp-content/plugins/hd_documentations/documentation/9.0/refman.pdf
#' for examples
#'
#' For example, to enable logging call:
#' gurobi_set_param("OutputFlag", "1")
#'
#' @examples
#' gurobi_set_param("TimeLimit", "100.0")
#'
gurobi_set_param <- function(param, value) {
    invisible(.Call(`_rcraam_gurobi_set_param`, param, value))
}

#'  Builds an MDP from samples
#'
mdp_from_samples <- function(samples_frame) {
    .Call(`_rcraam_mdp_from_samples`, samples_frame)
}

#' Constructs the linear programming matrix for the MDP
#'
#' The method can construct the LP matrix for the MDP, which is defined as
#' follows:
#' A = [I - gamma P_1; I - gamma P_2; ...] where P_a is the transition
#' probability for action_a. It also constructs the corresponding vector of rewards
#'
#'
#' @param mdp A dataframe representation of the MDP. Each row
#'            represents a single transition from one state to another
#'            after taking an action a. The columns are:
#'            idstatefrom, idaction, idstateto, probability, reward
#' @param discount Discount factor in [0,1]
#'
#' @return A list with entries A, b, and idstateaction which is a dataframe with
#'         row_index (1-based), stateid (0-based), actionid (0-based) that
#'         identifies the state and action for each row of the output
matrix_mdp_lp <- function(mdp, discount) {
    .Call(`_rcraam_matrix_mdp_lp`, mdp, discount)
}

#' Constructs transition probability matrix the MDP
#'
#' The method constructs the transition probability matrix (stochastic matrix)  P_pi
#' and rewards r_pi for the MDP
#'
#'
#' @param mdp A dataframe representation of the MDP. Each row
#'            represents a single transition from one state to another
#'            after taking an action a. The columns are:
#'            idstatefrom, idaction, idstateto, probability, reward
#' @param policy The policy used to construct the transition probabilities and rewards.
#'            It can be a deterministic policy, in which case it should be a dataframe
#'            with columns idstate and idaction. Both indices are 0-based.
#'            It can also be a randomized policy, in which case it should be a dataframe with
#'            columns idstate, idaction, probability.
#' @return A list with P and r, the transition matrix and the reward vector
matrix_mdp_transition <- function(mdp, policy) {
    .Call(`_rcraam_matrix_mdp_transition`, mdp, policy)
}

#' Whether Gurobi LP and MILP is installed
#' 
#' This function can be used when determining which functionality
#' is available in the package
rcraam_supports_gurobi <- function() {
    .Call(`_rcraam_rcraam_supports_gurobi`)
}

mdp_example <- function(name) {
    .Call(`_rcraam_mdp_example`, name)
}

mdp_inventory <- function(params) {
    .Call(`_rcraam_mdp_inventory`, params)
}

mdp_population <- function(capacity, initial, growth_rates_exp, growth_rates_std, rewards, external_mean, external_std, s_growth_model) {
    .Call(`_rcraam_mdp_population`, capacity, initial, growth_rates_exp, growth_rates_std, rewards, external_mean, external_std, s_growth_model)
}

#'
#' Simulates an MDP
#'
#' @param mdp Definition of the MDP
#' @param initial_state The index of the initial state
#' @param policy Assumes a randomized policy as the input
#' @param horizon How many steps to execute for each episode
#' @param episodes Number of episodes to run
#' @param seed Random number generator seed (a number)
#'
#' @return A dataframe with the states, actions, and rewards received
#'
simulate_mdp <- function(mdp, initial_state, policy, horizon, episodes, seed = NULL) {
    .Call(`_rcraam_simulate_mdp`, mdp, initial_state, policy, horizon, episodes, seed)
}


---
title: "RiverSwim_weighted_set"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r}
library(rcraam)
library(dplyr)
library(readr)
library(gtools)
library(reshape2)
loadNamespace("tidyr")
loadNamespace("reshape2")
loadNamespace("stringr")
source('utils.R')
source('experiment_helpers.R')
```


```{r}
## ----- Parameters --------
description <- "riverswim_mdp.csv"

init.dist <- rep(1/6,6)
discount <- 0.95
confidence <- 0.95
bayes.samples <- 1000

samples <- 50
sample.seed <- 2011
episodes <- 10
```

```{r}
## ----- Initialization ------

mdp.truth <- read_csv(description, 
                      col_types = cols(idstatefrom = 'i',
                                       idaction = 'i',
                                       idstateto = 'i',
                                       probability = 'd',
                                       reward = 'd'))
rewards.truth <- mdp.truth %>% select(-probability)
```


```{r}
# construct a biased policy to prefer going right
# this is to ensure that the "goal" state is sampled
ur.policy = data.frame(idstate = c(seq(0,5), seq(0,5)), 
                     idaction = c(rep(0,6), rep(1,6)),
                     probability = c(rep(0.2, 6), rep(0.8, 6)))

# compute the true value function
sol.true <- solve_mdp(mdp.truth, discount, show_progress = FALSE)
vf.true <- sol.true$valuefunction$value
```


```{r}
cat("True optimal return", vf.true %*% init.dist, "policy:", sol.true$policy$idaction, "\n\n")
```


```{r}
## ----- Generate Samples --------------

# generate samples from the swimmer domain
simulation <- simulate_mdp(mdp.truth, 0, ur.policy, episodes = episodes, 
                           horizon = samples, seed = sample.seed)


mdp.bayesian <- mdpo_bayes(simulation, rewards.truth, bayes.samples)

```

```{r}
## ---- Bayesian Credible Region -----

rmdp.bayesian <- function(mdp.bayesian){
# adjust the confidence level

  # provides a bound on the value function and the return
  sa.count <- nrow( unique(mdp.bayesian %>% select(idstatefrom, idaction)))
  confidence.rect <- (1-confidence)/sa.count


  # construct the mean bayesian model
  mdp.mean.bayes <- mdp.bayesian %>% group_by(idstatefrom, idaction, idstateto) %>%
    summarize(probability = mean(probability), reward = mean(reward))
  
  mean.probs <- mdp.mean.bayes %>% rename(probability_mean=probability) %>%
                  select(-reward)

  # compute L1 distances
  budgets <-
    inner_join(mean.probs,
               mdp.bayesian,
               by = c('idstatefrom', 'idaction', 'idstateto')) %>%
      mutate(diff = abs(probability - probability_mean)) %>%
      group_by(idstatefrom, idaction, idoutcome) %>%
      summarize(l1 = sum(diff)) %>%   group_by(idstatefrom, idaction) %>%
       summarize(budget = quantile(l1, 1-confidence.rect)) %>%
       rename(idstate = idstatefrom)

  return(list(mdp.mean = mdp.mean.bayes,
              budgets = budgets))
}

model.bayes.loc <- rmdp.bayesian(mdp.bayesian)
sol.bcr <- rsolve_mdp_sa(model.bayes.loc$mdp.mean, discount, "l1",
                         model.bayes.loc$budgets, show_progress = FALSE)
report_solution("Local BCR: ", mdp.bayesian, sol.bcr)

```



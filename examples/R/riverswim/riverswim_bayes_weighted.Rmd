---
title: "RiverSwim_weighted_set"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r}
library(rcraam)
library(dplyr)
library(readr)
library(gtools)
library(reshape2)
loadNamespace("tidyr")
loadNamespace("reshape2")
loadNamespace("stringr")
```


```{r}
## ----- Parameters --------
description <- "riverswim3_mdp.csv"

init.dist <- rep(1/6,6)
discount <- 0.99
confidence <- 0.95
bayes.samples <- 1000

samples <- 50
sample.seed <- 2011
episodes <- 10
```

```{r}
## ----- Initialization ------

mdp.truth <- read_csv(description, 
                      col_types = cols(idstatefrom = 'i',
                                       idaction = 'i',
                                       idstateto = 'i',
                                       probability = 'd',
                                       reward = 'd'))
rewards.truth <- mdp.truth %>% select(-probability)
```


```{r}
# construct a biased policy to prefer going right
# this is to ensure that the "goal" state is sampled
ur.policy = data.frame(idstate = c(seq(0,5), seq(0,5)), 
                     idaction = c(rep(0,6), rep(1,6)),
                     probability = c(rep(0.2, 6), rep(0.8, 6)))

# compute the true value function
sol.true <- solve_mdp(mdp.truth, discount, show_progress = FALSE)
vf.true <- sol.true$valuefunction$value
```


```{r}
cat("True optimal return", vf.true %*% init.dist, "policy:", sol.true$policy$idaction, "\n\n")
```


```{r}
## ----- Generate Samples --------------

# generate samples from the swimmer domain
simulation <- simulate_mdp(mdp.truth, 0, ur.policy, episodes = episodes, 
                           horizon = samples, seed = sample.seed)

```

```{r}
## ----  Uninformative Bayesian Posterior Sampling ---------

#' Generate a sample MDP from dirichlet distribution
#' @param simulation Simulation results
#' @param rewards.df Rewards for each idstatefrom, idaction, idstateto
#' @param outcomes Number of outcomes to generate
mdpo_bayes <- function(simulation, rewards.df, outcomes){
  priors <- rewards.df %>% select(-reward) %>% unique() 
  # compute sampled state and action counts
  # add a uniform sample of each state and action to work as the dirichlet prior
  sas_post_counts <- simulation %>% 
    select(idstatefrom, idaction, idstateto) %>%
    rbind(priors) %>%
    group_by(idstatefrom, idaction, idstateto) %>% 
    summarize(count = n()) 

  # construct dirichlet posteriors
  posteriors <- sas_post_counts %>% 
    group_by(idstatefrom, idaction) %>% 
    arrange(idstateto) %>% 
    summarize(posterior = list(count), idstatesto = list(idstateto)) 
  
  # draw a dirichlet sample
  trans.prob <- 
    mapply(function(idstatefrom, idaction, posterior, idstatesto){
      samples <- do.call(function(x) {rdirichlet(outcomes,x)}, list(posterior) )
      # make sure that the dimensions are named correctly
      dimnames(samples) <- list(seq(0, outcomes-1), idstatesto)
      reshape2::melt(samples, varnames=c('idoutcome', 'idstateto'), 
                     value.name = "probability" ) %>%
        mutate(idstatefrom = idstatefrom, idaction = idaction)
    },
    posteriors$idstatefrom,
    posteriors$idaction,
    posteriors$posterior,
    posteriors$idstatesto,
    SIMPLIFY = FALSE)
  
  mdpo <- bind_rows(trans.prob) %>% 
    full_join(rewards.df, 
              by = c('idstatefrom', 'idaction','idstateto')) %>%
    na.fail()
  return(mdpo)
}

mdp.bayesian <- mdpo_bayes(simulation, rewards.truth, bayes.samples)

```

```{r}
#' Evaluate the policy with respect Bayesian outcomes. 
#' 
#' Returns the return values.
#' 
#' @param mdp.bayesion MDPO with outcomes
#' @param policy Deterministic policy to be evaluated
bayes.returns <- function(mdp.bayesian, policy, maxcount = 100){
  outcomes.unique <- head(unique(mdp.bayesian$idoutcome),maxcount)
  maxcount <- min(maxcount, nrow(outcomes.unique))
  sapply(outcomes.unique,
         function(outcome){
           sol <- mdp.bayesian %>% filter(idoutcome == outcome) %>% 
             solve_mdp(discount, policy_fixed = policy, 
                       show_progress = FALSE, algorithm = "pi")          
           sol$valuefunction$value %*% init.dist
         })
}
```


```{r}

#' Prints experiment result statistics.
#' 
#' It also prints its guarantees, solution quality and 
#' posterior expectation of how well it is likely to work
#' 
#' @param name Name of the algorithm that produced the results
#' @param mdp.bayesian MDP with outcomes representing bayesian samples
#' @param solution Output from the algorithm's solution
report_solution <- function(name, mdp.bayesian, solution){
  predicted <- ifelse("valuefunction" %in% ls(solution), 
                      solution$valuefunction$value %*% init.dist,
                      solution$objective)
  cat("**", stringr::str_pad(name, 15, 'right'), predicted, "****\n")
  cat("    Policy", solution$policy$idaction,"\n")
  cat("    Return predicted:", predicted)
  sol.tr <- solve_mdp(mdp.truth, discount, 
                      policy_fixed = solution$policy,
                      show_progress = FALSE)
  cat(", true:", sol.tr$valuefunction$value %*% init.dist, "\n")
  posterior.returns <- bayes.returns(mdp.bayesian, solution$policy)
  dst <- rep(1/length(posterior.returns), length(posterior.returns))
  cat("    Posterior mean:", mean(posterior.returns), ", v@r:", 
      quantile(posterior.returns, 1-confidence), ", av@r:", 
      avar(posterior.returns, dst, 1-confidence)$value)
  cat("\n")
}
```

```{r}
## ---- Solve Empirical MDP ---------

mdp.empirical <- mdp_from_samples(simulation)
sol.empirical <- solve_mdp(mdp.empirical, discount, show_progress = FALSE)
report_solution("Empirical", mdp.bayesian, sol.empirical)
```


```{r}
## ----- Solve Bayesian MDP ---------
sol.bayesexp <- rsolve_mdpo_sa(mdp.bayesian, discount, "exp", NULL, show_progress = FALSE)
report_solution("Bayesian", mdp.bayesian, sol.bayesexp)
```

```{r}
## ----- Frequentist Confidence Interval ------

#' Constructs an MDP and a confidence interval
#' See e.g. Russel 2019
#' Confidence is: sqrt{ 1 / n_{s,a}  log (S A 2^S / \delta) }
#' where delta is the confidence
#' 
#' Assumes that only transitions that have rewards associates with them are possible
#' @param simulation Samples from the simulations
#' @param rewards.df Like an MDP description, just with the rewards only 
#'                   and no probabilites. Used to determine rewards and 
#'                   which transitions are possible (and considerd by the robust sol)
rmdp.frequentist <- function(simulation, rewards.df){
  mdp.nominal <- mdp_from_samples(simulation)
  
  # count the number of samples for each state and action
  sa_counts <- simulation %>% select(idstatefrom, idaction) %>%
    group_by(idstatefrom, idaction) %>% summarize(count = n())
  
  sa.count <- nrow(sa_counts)                   # number of valid state-action pairs
  # count the number of possible transitions from each state and action
  tran.count <- rewards.truth %>% group_by(idstatefrom, idaction) %>% 
    summarize(tran_count = n())
  
  budgets <- full_join(sa_counts, tran.count, by = c('idstatefrom','idaction')) %>% 
    mutate(budget = coalesce( 
      pmin(2.0,sqrt(1/count * log(sa.count * 2^tran_count / confidence))),
      2.0)) %>%
    rename(idstate = idstatefrom) %>% select(-count) %>% na.fail()
  
  # add transtions to states that have not been observed, and normalize
  # them in order to get some kind of a transition probability
  mdp.nominal <- full_join(mdp.nominal %>% select(-reward), rewards.df, 
                           by = c('idstatefrom', 'idaction', 'idstateto')) %>% 
    mutate(probability = coalesce(probability, 1.0))
  # normalize transition probabilities
  mdp.nominal <-
    full_join(mdp.nominal %>% group_by(idstatefrom, idaction) %>%
                summarize(prob.sum = sum(probability)),
              mdp.nominal, by=c('idstatefrom', 'idaction')) %>%
    mutate(probability = probability / prob.sum) %>% select(-prob.sum) %>% na.fail()
  
  return (list(mdp.nominal = mdp.nominal, budgets = budgets))
}

model.freq <- rmdp.frequentist(simulation, rewards.truth)
sol.freq <- rsolve_mdp_sa(model.freq$mdp.nominal, discount, "l1", model.freq$budgets, 
                          show_progress = FALSE)

report_solution("Hoeff CR", mdp.bayesian, sol.freq)

```


